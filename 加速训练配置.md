# 加速训练配置

### 方案一：GPU 加速（推荐）

你需要修改 `xgboost` 模型的初始化参数，告诉它使用 GPU。

1.  **打开文件**：`utils/models.py`

2.  **修改三处**：你需要在这个文件中找到所有 `xgb.XGBRegressor(...)` 被调用的地方，并添加 `tree_method='gpu_hist'` 和 `device='cuda'`。

      * **第一次修改 (train\_and\_predict 函数, 约 221 行):**

          * **修改前:**
            ```python
            regr = xgb.XGBRegressor(max_depth=self.max_depth, n_estimators=self.n_estimators, random_state=ensemble_state+self.n_split)
            ```
          * **修改后:**
            ```python
            regr = xgb.XGBRegressor(max_depth=self.max_depth, n_estimators=self.n_estimators, random_state=ensemble_state+self.n_split,
                                    tree_method='gpu_hist', device='cuda')
            ```

      * **第二次修改 (train\_and\_predict\_vd2 函数, 约 300 行):**

          * **修改前:**
            ```python
            regr = xgb.XGBRegressor(max_depth=self.max_depth, n_estimators=self.n_estimators, random_state=ensemble_state+self.n_split)
            ```
          * **修改后:**
            ```python
            regr = xgb.XGBRegressor(max_depth=self.max_depth, n_estimators=self.n_estimators, random_state=ensemble_state+self.n_split,
                                    tree_method='gpu_hist', device='cuda')
            ```

      * **第三次修改 (train\_no\_predict 函数, 约 374 行):**

          * **修改前:**
            ```python
            regr = xgb.XGBRegressor(max_depth=self.max_depth, n_estimators=self.n_estimators, random_state=ensemble_state+self.n_split)
            ```
          * **修改后:**
            ```python
            regr = xgb.XGBRegressor(max_depth=self.max_depth, n_estimators=self.n_estimators, random_state=ensemble_state+self.n_split,
                                    tree_method='gpu_hist', device='cuda')
            ```

**前提条件：**

  * 你必须有一块 NVIDIA 显卡。
  * 你的 `xgboost` 库（你安装的是 2.1.3）已经内置了 GPU 支持，但你本地的 NVIDIA 驱动和 CUDA Toolkit 版本需要与 `xgboost` 兼容。通常，你只需要保持你的 NVIDIA 驱动是最新版即可。

-----

### 方案二：CPU 加速（多核利用）

`xgboost` 默认就会尝试使用你 CPU 的所有核心。你可以通过添加 `n_jobs=-1` 来确保这一点（和 GPU 方案的修改位置一样）。

```python
regr = xgb.XGBRegressor(..., n_jobs=-1)
```

但是，既然你已经觉得慢，说明你很可能**已经**在用满 CPU 了。

-----

### 方案三：降低计算量（用于快速调试）

如果你只是想快速跑通流程，看看结果，而不是进行最终的论文复现，**最好的办法是减少工作量**。

打开你正在运行的实验脚本（例如 `experiments/1next-cycle-capacity.py`），修改 `params` 字典：

  * **修改前:**
    ```python
    params = {'max_depth':100,
              'n_splits':12,
              'n_estimators':500,
              'n_ensembles':10}
    ```
  * **修改后 (示例，用于快速测试):**
    ```python
    params = {'max_depth':10,    # 深度从100降到10
              'n_splits':2,      # 12折交叉验证改成2折
              'n_estimators':25, # 500棵树改成25棵
              'n_ensembles':2}     # 10个模型改成2个
    ```
      * `1variable-discharge-train.py` 用的就是这个小参数集，你可以参考它。

**总结：**
我强烈推荐你**优先使用方案一（GPU 加速）**，这是解决你问题的最直接、最正确的方法。